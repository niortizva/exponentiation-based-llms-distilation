\section{Conclusion and future directions}

The fundamental goal of this work was to design an efficient attention mechanism without losing the state-of-the-art complexity abstraction of standard multi-head attention. By designing a new matrix exponentiation formulation, we were able to reduce the computational complexity compuation of attention while maintaining expressivity. Our experiments demonstrate that the proposed method is able to compress attention computation $257\times$ more than standard attention, making it suitable for long sequence tasks on transformer architectures. We also prove some useful properties that can be used to define the exponential function and other related functions for non-square matrices, opening new research directions in the field of matrix functions and attention-based architectures. Moreover, we believe that our work can be extended in several ways. Future research could explore the integration of our efficient attention mechanism with other transformer variants, such as sparse attention or adaptive attention mechanisms. Additionally, investigating the impact of our method on different modalities, such as vision or audio, could provide further insights into its versatility and effectiveness. Finally, exploring theoretical aspects of matrix functions in the context of deep learning could lead to new advancements in model design and optimization techniques. For example, one could think of exploring whether the proposed attention gradient is bounded, such that gradient explosions are avoided during training. Also, investigating the convergence properties of models utilizing our attention mechanism could yield valuable theoretical insights. Overall, our work lays a solid foundation for future explorations in efficient attention mechanisms and their applications across various domains. \\

In conclusion, we have presented a novel approach to efficient attention computation that significantly reduces complexity while preserving the expressiveness of traditional multi-head attention mechanisms. Our method not only advances the state-of-the-art in attention mechanisms but also opens up new avenues for research in matrix functions and their applications in deep learning. 

\section*{Acknowledgements}

We would like to express our sincere gratitude to all those who contributed to this research. Special thanks to our colleagues and mentors for their invaluable feedback and support throughout the development of this work. We also acknowledge a PhD. Santiago Herce Castañón and PhD Student Fernando Avitúa Varela for the opportunity to discuss our work at 'Centro de la Complejidad UNAM', which provided us with new perspectives and insights. Finally, we extend our appreciation to the National Polytechnic Intitute and CONACYT funding that supported this research, enabling us to pursue our goals and contribute to the field of efficient attention mechanisms.