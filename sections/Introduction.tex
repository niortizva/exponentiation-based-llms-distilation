\section{Introduction}

Polynomial Neural Networks (PNNs) demonstrate the capability of learning complex feature representations through hierarchical function composition. Concurrently, the multi-head attention mechanism has been established as a cornerstone of modern Large Language Models (LLMs), enabling them to capture diverse contextual relationships. However, the computational and parametric cost of multi-head attention presents a significant bottleneck for resource-constrained applications. This work investigates a novel exponentiation-based attention approach, inspired by the functional form of multi-head attention, with the goal of capturing rich, high-order feature interactions without the prohibitive cost associated with standard LLM architectures.