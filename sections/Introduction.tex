\begin{abstract}
    Large Language Models (LLMs) have revolutionized natural language processing, yet their deployment is often hindered by substantial computational and memory demands, particularly due to the multi-head attention mechanism. This paper introduces NSEA-Attention, a novel exponentiation-based attention mechanism designed to capture high-order feature interactions while significantly reducing parameter count. We formulate a generalized matrix exponentiation operation for non-square matrices, constructing an attention mechanism, which can emulates the information mixing behavior of standard attention with enhanced efficiency. Our experiments demonstrate a substantial reduction in model size and computation overhead of about $257\times$. This work paves the way for more efficient LLMs, facilitating their deployment in resource-constrained environments without compromising performance.
\end{abstract}

\section{Introduction}

Attention-based mechanisms have been established as a cornerstone of modern Large Language Models (LLMs), enabling them to capture diverse contextual relationships. However, the computational and parametric cost of multi-head attention presents a significant bottleneck in the development of LLMs.
Its canonical formulation computes a pairwise affinity matrix over all input tokens, leading to quadratic complexity \(O(n^2)\) in both computation and memory with respect to sequence length \(n\)~\citep{Niu2021}. 
Despite modern advances in efficient attention computation, which lead to linear complexity ~\citep{deepseekai2025deepseekv3technicalreport}, the number of parameters and overall compute remains a critical impediment for training on long contexts and, more pertinently, for the deployment of these models in resource-constrained environments. \\

In response, compression and acceleration techniques have emerged as a strategy for compressing large LLMs into smaller, faster, and computationally efficient models, like knowledge distillation in TinyBERT, which achieves more than 96.8\% performance of its teacher BERTBASE on GLUE benchmark~\citep{Wang2018}, while being \textbf{7.5x} smaller and \textbf{9.4x} faster on inference~\citep{Jiao2020}. 
Common and powerful model compression and acceleration techniques include: parameter prunning and sharing; removing inessential parameters without any significant effect on the model performace, low-rank factorization; identify redundant parameters and decompose then into smaller matrices, convolutional filters transferring; removing inessential parameters by transferring or compressing convolutional filters, and knowledge distillation (KD); distilling knowledge from larger models into small models~\citep{Gou2021}. 
However, these strategies remain limited by the inherent inefficiencies of the attention mechanism itself, which is typically retained in its original form within the student model~\citep{Pan2021,Jiao2020}. 
Additionally, modeling different types of knowledge in a unified and complementary framework is still challenging; small models do not capture all complex language relationships associated with their homologous large versions~\citep{Gou2021}.\\

To address these challenges, this work investigates a novel exponentiation-based attention approach, inspired by the functional form of multi-head attention, with the goal of capturing rich, high-order feature interactions without the prohibitive cost associated with standard LLM architectures.
We propose a fundamentally new approach to attention, which we term NSEA-Attention for Non-square Matrix Product Exponentiation-based Attention. 
Our key insight is to reconceptualize attention not as a static affinity computation, but as a parameterized sequence of non-square matrix multiplications that can effectively emulate the information mixing behavior of standard attention and capture complex relationships, while being significantly more efficient in terms of parameters. 
For this end, we design a matrix product exponentiation process, \( \exp\left( (\X \W + \B)^2 \mathbf{\Lambda} \right) \), where \(W\) and \(\mathbf{\Lambda}\) are deliberately non-square matrices that project input tokens into asymmetric latent spaces.
This exponentiation process inherently encodes a form of exponential decay or amplification of learned relationships over \(k\) steps (Taylor series expansion approximation $\exp(\A) = \I{m}{n} + \sum_{k=1}^{\infty} \frac{\A^k}{k!}$), mimicking the way standard attention propagates influence, but doing so through a deep, compositional parameterized pathway. 
We hypothesize that this allows controlling specialized representation shaping, enabling the model to learn a compressed yet semantically rich attention mechanism. By doing so, we can reduce the number of Attention layers and, in consequence, reduce the number of model parameters without lossing model's modeling capabilities.\\

The remainder of this article is structured as follows: We first provide necessary background on attention. We then detail the mathematical formulation and architecture of NSEA-Attention. Following this, we present our proposed methodology. Experimental results demonstrate the efficacy of our approach in creating a highly efficient, high-performing attention mechanism. We conclude with an analysis and a discussion of future work.