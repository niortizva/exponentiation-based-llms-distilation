\section{Experiments and Results}

To evaluate the efficiency of the NSEA mechanism, we compare its performance against traditional multi-head attention. 

\subsubsection{Computation Effeciency}

We analyszed the computation effeciency of NSEA compared against Multi-head attention (MHA), by measuring the FLOPs taken for forward passes on varying vocab sizes and batch length for a random input sequence of lengh $16384 = (4096 \times 4)$, an wmbedding size of $2048$ (DeeepSeek's maximm sequence length and embedding size \citep{deepseekai2025deepseekv3technicalreport}), and a $4$-degree Taylor aproxximation on a computer with $24$ GB of RAM with a speed of $4800 MT/s$, and a $13^{th}$ Generation Intel(R) Core(TM) i5-3420H. Figure~\ref{fig:computation_efficiency} illustrates the testing setup.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/computation_efficiency_setup.png}
    \caption{Computation Efficiency Testing Setup}
    \label{fig:computation_efficiency}
\end{figure}

\input{sections/tables/computation_efficiency}

Table~\ref{tab:computation_efficiency} indicates that HEA outperforms MHA. Results prove that NSEA is significantly more efficient than MHA in terms of FLOPs, with a compression ratio of approximately $257$ times. This efficiency gain is attributed to the reduced computational complexity of the non-square exponential operation compared to the traditional attention mechanism, which involves more intensive matrix multiplications and softmax operations.
