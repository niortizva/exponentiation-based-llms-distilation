@misc{ying2021lazyformerselfattentionlazy,
      title={LazyFormer: Self Attention with Lazy Update}, 
      author={Chengxuan Ying and Guolin Ke and Di He and Tie-Yan Liu},
      year={2021},
      eprint={2102.12702},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2102.12702}, 
}

@article{Esearch2011,
   abstract = {This research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analytics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.},
   author = {S Ystems R Esearch and Otto R Koppius},
   issue = {3},
   journal = {MIS Quarterly},
   title = {Predictive Attention Transformer: Improving Transformer with Attention Map Prediction},
   volume = {35},
   year = {2011}
}

@article{Raffel2020,
   abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
   author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
   volume = {21},
   year = {2020}
}


@article{Guo2019rank,
   abstract = {Self-attention mechanism becomes more and more popular in natural language processing (NLP) applications. Recent studies show the Transformer architecture which relies mainly on the attention mechanism achieves much success on large datasets. But a raised problem is its generalization ability is weaker than CNN and RNN on many moderate-sized datasets. We think the reason can be attributed to its unsuitable inductive bias of the self-attention structure. In this paper, we regard the self-attention as matrix decomposition problem and propose an improved self-attention module by introducing two linguistic constraints: low-rank and locality. We further develop the low-rank attention and band attention to parameterize the self-attention mechanism under the low-rank and locality constraints. Experiments on several real NLP tasks show our model outperforms the vanilla Transformer and other self-attention models on moderate size datasets. Additionally, evaluation on a synthetic task gives us a more detailed understanding of working mechanisms of different architectures.},
   author = {Qipeng Guo and Xipeng Qiu and Xiangyang Xue and Zheng Zhang},
   doi = {10.1109/TASLP.2019.2944078},
   issn = {23299304},
   issue = {12},
   journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
   title = {Low-Rank and Locality Constrained Self-Attention for Sequence Modeling},
   volume = {27},
   year = {2019}
}


@inproceedings{Choromanski2022,
   abstract = {In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation.},
   author = {Krzysztof Choromanski and Han Lin and Haoxian Chen and Tianyi Zhang and Arijit Sehanobish and Valerii Likhosherstov and Jack Parker-Holder and Tamas Sarlos and Adrian Weller and Thomas Weingarten},
   issn = {26403498},
   booktitle = {Proceedings of Machine Learning Research},
   title = {From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers},
   volume = {162},
   year = {2022}
}

@inproceedings{Katharopoulos2020,
   abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-Attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O-N2 to O(N), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
   author = {Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and Francois Fleuret},
   booktitle = {37th International Conference on Machine Learning, ICML 2020},
   title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
   volume = {PartF168147-7},
   year = {2020}
}

@article{Feng2022,
   author = {Yuchao Feng and Honghui Xu and Jiawei Jiang and Hao Liu and Jianwei Zheng},
   doi = {10.1109/TGRS.2022.3168331},
   issn = {15580644},
   journal = {IEEE Transactions on Geoscience and Remote Sensing},
   title = {ICIF-Net: Intra-Scale Cross-Interaction and Inter-Scale Feature Fusion Network for Bitemporal Remote Sensing Images Change Detection},
   volume = {60},
   year = {2022}
}

@misc{child2019generatinglongsequencessparse,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.10509}, 
}

@article{Guo2019,
   abstract = {The classification performance of aerial scenes relies heavily on the discriminative power of feature representation from high-spatial resolution remotely sensed imagery. The convolutional neural networks (CNNs) have recently been applied to adaptively learn image features at different levels of abstraction rather than requiring handcrafted features and achieved state-of-the-art performance. However, most of these networks focus on multi-stage global feature learning yet neglect the local information, which plays an important role in scene recognition. To address this issue, a novel end-to-end global-local attention network (GLANet) is proposed to capture both global and local information for aerial scene classification. FC layers in the VGGNet are replaced by the global attention (GA) branch and local attention (LA) branch, one of which learns the global information while the other learns the local semantic information via attention mechanisms. During each training, the labels of input images can be predicted by the local, global, and their concatenated features using softmax. According to different predicted labels, two auxiliary loss functions are further computed and imposed on the proposed network to enhance the supervision for network learning. The experimental results on three challenging large-scale scene datasets demonstrate the effectiveness of the proposed global-local attention network.},
   author = {Yiyou Guo and Jinsheng Ji and Xiankai Lu and Hong Huo and Tao Fang and Deren Li},
   doi = {10.1109/ACCESS.2019.2918732},
   issn = {21693536},
   journal = {IEEE Access},
   title = {Global-Local Attention Network for Aerial Scene Classification},
   volume = {7},
   year = {2019}
}

@article{Xu2021,
   abstract = {Cataracts are the most crucial cause of blindness among all ophthalmic diseases. Convenient and cost-effective early cataract screening is urgently needed to reduce the risks of visual loss. To date, many studies have investigated automatic cataract classification based on fundus images. However, existing methods mainly rely on global image information while ignoring various local and subtle features. Notably, these local features are highly helpful for the identification of cataracts with different severities. To avoid this disadvantage, we introduce a deep learning technique to learn multilevel feature representations of the fundus image simultaneously. Specifically, a global–local attention network (GLA-Net) is proposed to handle the cataract classification task, which consists of two levels of subnets: the global-level attention subnet pays attention to the global structure information of the fundus image, while the local-level attention subnet focuses on the local discriminative features of the specific regions. These two types of subnets extract retinal features at different attention levels, which are then combined for final cataract classification. Our GLA-Net achieves the best performance in all metrics (90.65% detection accuracy, 83.47% grading accuracy, and 81.11% classification accuracy of grades 1 and 2). The experimental results on a real clinical dataset show that the combination of global-level and local-level attention models is effective for cataract screening and provides significant potential for other medical tasks.},
   author = {Xi Xu and Jianqiang Li and Yu Guan and Linna Zhao and Qing Zhao and Li Zhang and Li Li},
   doi = {10.1016/j.jbi.2021.103939},
   issn = {15320464},
   journal = {Journal of Biomedical Informatics},
   title = {GLA-Net: A global-local attention network for automatic cataract classification},
   volume = {124},
   year = {2021}
}

@article{Xiong2025,
   abstract = {Current methods synthetic aperture radar-automatic target recognition (SAR-ATR) research methods still struggle with overfitting due to small amounts of training data, as well as black-box opacity and high computational requirements. Unmanned aerial vehicles, as the mainstream means of acquiring SAR data, place higher requirements on ATR algorithms due to their flexible maneuvering characteristics. This article starts by studying the electromagnetic (EM) backscattering mechanism and the physical properties of SAR. We construct a heterogeneous graph for the first time to fully exploit both the EM scattering information of the target components and their interactions. Moreover, the multilevel multihead attention mechanism is introduced to the graph net to learn features from various topological structure levels. Additionally, we include a convolutional neural network based feature extraction net to replenish intuitive visual features. The above two nets form the lightweight dual-stream framework (LDSF). LDSF uses a feature fusion subnetwork to adaptively fuse the dual-stream features to maximize the final classification performance. The experiments use two more rigorous evaluation protocols on MSTAR and OpenSARShip, namely, once-for-all and less-for-more, which can rigorously assess the efficacy and generalization capability of the algorithms. The superiority of LDSF is verified.},
   author = {Xuying Xiong and Xinyu Zhang and Weidong Jiang and Tianpeng Liu and Yongxiang Liu and Li Liu},
   doi = {10.1109/JSTARS.2024.3498327},
   issn = {21511535},
   journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
   title = {Lightweight Dual-Stream SAR-ATR Framework Based on an Attention Mechanism-Guided Heterogeneous Graph Network},
   volume = {18},
   year = {2025}
}
@article{Zhang2025,
   abstract = {Aerial-based person detection poses a significant challenge, yet it is crucial for real-world applications like air-ground linkage search and all-weather intelligent corescuing. However, existing person detection models designed for aerial images heavily rely on numerous labeled instances and exhibit limited tolerance towards complex lighting conditions commonly encountered in search and rescue (SaR) scenarios. This article presents the visible-thermal from SaR scenarios for person detection network (VTSaRNet) to address the challenge of detecting persons situated sparsely in SaR scenes marked by intricate illumination conditions and restricted accessibility. VTSaRNet integrates the instance segmentation for copy-paste mechanism (ISCP) using a Union Transformer Network that functions in both Visible (V) and Thermal (T) bimodalities. Specifically, This study employs synthetic samples obtained through offline Mosaic augmentation by oversampling the local area of bulk images. Then, it utilizes the ISCP module to extract accurate boundaries of personnel instances from complex backgrounds. VTSaRNet cross-integrates the global features and encodes the correlations between two modalities through the multihead attention module. It also adaptively recalibrates the channel responses of partial feature maps for fusion operations with the transformer module in conjunction with anchor-based detectors. Moreover, the adaptation scheme is constructed with multiple strategies to effectively handle various scenarios involving persons, and the entire network is trained end-to-end. Extensive experiments conducted on the Heridal and VTSaR datasets demonstrate the effectiveness of light-weighted VTSaRNet in achieving impressive metrics precision of 98.3%, recall of 96.78%, mAP@0.5 of 98.73%, and mAP@0.5:0.95 of 73.98% under self-built VTSaR dataset, respectively). This performance sets a new benchmark in person detection from aerial imagery.},
   author = {Xiangqing Zhang and Yan Feng and Nan Wang and Guohua Lu and Shaohui Mei},
   doi = {10.1109/JSTARS.2025.3526995},
   issn = {21511535},
   journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
   title = {Transformer-Based Person Detection in Paired RGB-T Aerial Images with VTSaR Dataset},
   volume = {18},
   year = {2025}
}
@article{Liu2025,
   abstract = {Object detection is a fundamental capability that enables drones to perform various tasks. However, achieving a suitable equilibrium between performance, efficiency, and lightweight design continues to be a significant challenge for current algorithms. To address this issue, we propose an enhanced small object detection transformer model called ESO-DETR. First, we present a gated single-head attention backbone block, known as the GSHA block, which enhances the extraction of local details. Besides, ESO-DETR utilizes the multiscale multihead self-attention mechanism (MMSA) to efficiently manage complex features within its backbone network. We also introduce a novel and efficient feature fusion pyramid network for enhanced small object detection, termed ESO-FPN. This network integrates large convolutional kernels with dual-domain attention mechanisms. Lastly, we introduce the EMASlideVariFocal loss (ESVF Loss), which dynamically adjusts the weights to improve the model’s focus on more challenging samples. In comparison with the baseline model, ESO-DETR demonstrates enhancements of 3.9% and 4.0% in the (Formula presented.) metric on the VisDrone and HIT-UAV datasets, respectively, while also reducing parameters by 25%. These results highlight the capability of ESO-DETR to improve detection accuracy while maintaining a lightweight and efficient structure.},
   author = {Yingfan Liu and Miao He and Bin Hui},
   doi = {10.3390/drones9020143},
   issn = {2504446X},
   issue = {2},
   journal = {Drones},
   title = {ESO-DETR: An Improved Real-Time Detection Transformer Model for Enhanced Small Object Detection in UAV Imagery},
   volume = {9},
   year = {2025}
}

@article{Li2025,
   abstract = {Hyperspectral imaging (HSI) can capture a large amount of spectral information at various wavelengths, enabling detailed material classification and identification, making it a key tool in remote sensing, particularly for coastal area monitoring. In recent years, the convolutional neural network (CNN) framework and transformer models have demonstrated strong performance in HSI classification, especially in applications requiring precise change detection and analysis. However, due to the high dimensionality of HSI data and the complexity of spectral-spatial feature extraction, achieving accurate results in coastal areas remains challenging. This article introduces a new hybrid model, CSTFNet, which combines an improved CNN module and dual-layer Swin transformer (DLST) to tackle these challenges. CSTFNet integrates spectral and spatial processing capabilities, significantly reducing computational complexity while maintaining high classification accuracy. The improved CNN module employs one-dimensional convolutions to handle high-dimensional data, while the DLST module uses window-based multihead attention to capture both local and global dependencies. Experiments conducted on four standard HSI datasets (Houston-2013, Samson, KSC, and Botswana) demonstrate that CSTFNet outperforms traditional and state-of-the-art algorithms, achieving overall classification accuracy exceeding 99%. In particular, on the Houston-2013 dataset, the results for OA and AA are 1.00 and the kappa coefficient is 0. 976. The results highlight the robustness and efficiency of the proposed model in coastal area applications, where accurate and reliable spectral-spatial classification is crucial for monitoring and environmental management.},
   author = {Dekai Li and Harold Neira-Molina and Mengxing Huang and M. S. Syam and Yu Zhang and Zhang Junfeng and Uzair Aslam Bhatti and Muhammad Asif and Nadia Sarhan and Emad Mahrous Awwad},
   doi = {10.1109/JSTARS.2025.3530935},
   issn = {21511535},
   journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
   title = {CSTFNet: A CNN and Dual Swin-Transformer Fusion Network for Remote Sensing Hyperspectral Data Fusion and Classification of Coastal Areas},
   volume = {18},
   year = {2025}
}

@article{Sharifi2025,
   abstract = {Satellite imagery plays a pivotal role in environmental monitoring, urban planning, and national security. However, spatial resolution limitations of current satellite sensors restrict the clarity and usability of captured images. This study introduces a novel transformer-based deep-learning model to enhance the spatial resolution of Sentinel-2 images. The proposed architecture leverages multihead attention and integrated spatial and channel attention mechanisms to effectively extract and reconstruct fine details from low-resolution inputs. The model's performance was evaluated on the Sentinel-2 dataset, along with benchmark datasets (AID and UC-Merced), and compared against state-of-the-art methods, including ResNet, Swin Transformer, and ViT. Experimental results demonstrate superior performance, achieving a peak signal-to-noise ratio (PSNR) of 33.52 dB, structural similarity index (SSIM) of 0.862, and signal-to-reconstruction error ratio (SRE) of 36.7 dB on Sentinel-2 RGB bands. The proposed method outperforms state-of-the-art approaches, including ResNet, Swin Transformer, and ViT, on benchmark datasets (Sentinel-2, AID, and UC-Merced). The results demonstrate that the proposed method achieves superior performance in terms of PSNR, SSIM, and SRE metrics, highlighting its effectiveness in revealing finer spatial details and improving image quality for practical remote sensing applications.},
   author = {Alireza Sharifi and Mohammad Mahdi Safari},
   doi = {10.1109/JSTARS.2025.3526260},
   issn = {21511535},
   journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
   title = {Enhancing the Spatial Resolution of Sentinel-2 Images Through Super-Resolution Using Transformer-Based Deep-Learning Models},
   volume = {18},
   year = {2025}
}

@article{Hasan2024,
   abstract = {Accurate gestational age (GA) prediction is crucial for monitoring fetal development and ensuring optimal prenatal care. Traditional methods often face challenges in terms of precision and prediction efficiency. In this context, leveraging modern deep learning (DL) techniques is a promising solution. This paper introduces a novel DL approach for GA prediction using fetal brain images obtained via magnetic resonance imaging (MRI), which combines the strength of the Xception pretrained model with a multihead attention (MHA) mechanism. The proposed model was trained on a diverse dataset comprising 52,900 fetal brain images from 741 patients. The images encompass a GA ranging from 19 to 39 weeks. These pretrained models served as feature extraction components during the training process. The extracted features were subsequently used as the inputs of different configurable MHAs, which produced GA predictions in days. The proposed model achieved promising results with 8 attention heads, 32 dimensionality of the key space and 32 dimensionality of the value space, with an R-squared (R2) value of 96.5 %, a mean absolute error (MAE) of 3.80 days, and a Pearson correlation coefficient (PCC) of 98.50 % for the test set. Additionally, the 5-fold cross-validation results reinforce the model's reliability, with an average R2 of 95.94 %, an MAE of 3.61 days, and a PCC of 98.02 %. The proposed model excels in different anatomical views, notably the axial and sagittal views. A comparative analysis of multiple planes and a single plane highlights the effectiveness of the proposed model against other state-of-the-art (SOTA) models reported in the literature. The proposed model could help clinicians accurately predict GA.},
   author = {Mohammad Asif Hasan and Fariha Haque and Tonmoy Roy and Mahedi Islam and Md Nahiduzzaman and Mohammad Mahedi Hasan and Mominul Ahsan and Julfikar Haider},
   doi = {10.1016/j.compbiomed.2024.109155},
   issn = {18790534},
   journal = {Computers in Biology and Medicine},
   title = {Prediction of fetal brain gestational age using multihead attention with Xception},
   volume = {182},
   year = {2024}
}

@article{Bahdanau2015Rnn,
   abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
   author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
   journal = {Computer Science},
   title = {RNN Encoder-Decoder with Attention},
   year = {2015}
}

@article{Bahdanau2015Seq,
   abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
   author = {Dzmitry Bahdanau and Kyung Hyun Cho and Yoshua Bengio},
   journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
   title = {seq2seq + attention},
   year = {2015}
}

@article{deSantanaCorreia2022,
   abstract = {In humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last 6 years, this property has been widely explored in deep neural networks. Currently, the state-of-the-art in Deep Learning is represented by neural attention models in several application domains. This survey provides a comprehensive overview and analysis of developments in neural attention models. We systematically reviewed hundreds of architectures in the area, identifying and discussing those in which attention has shown a significant impact. We also developed and made public an automated methodology to facilitate the development of reviews in the area. By critically analyzing 650 works, we describe the primary uses of attention in convolutional, recurrent networks, and generative models, identifying common subgroups of uses and applications. Furthermore, we describe the impact of attention in different application domains and their impact on neural networks’ interpretability. Finally, we list possible trends and opportunities for further research, hoping that this review will provide a succinct overview of the main attentional models in the area and guide researchers in developing future approaches that will drive further improvements.},
   author = {Alana de Santana Correia and Esther Luna Colombini},
   doi = {10.1007/s10462-022-10148-x},
   issn = {15737462},
   issue = {8},
   journal = {Artificial Intelligence Review},
   title = {Attention, please! A survey of neural attention models in deep learning},
   volume = {55},
   year = {2022}
}

@article{Lin2022,
   abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
   author = {Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
   doi = {10.1016/j.aiopen.2022.10.001},
   issn = {26666510},
   journal = {AI Open},
   title = {A survey of transformers},
   volume = {3},
   year = {2022}
}

@inproceedings{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
   doi = {10.1201/9781003561460-19},
   issn = {10495258},
   booktitle = {Advances in Neural Information Processing Systems},
   title = {Attention is all you need},
   volume = {2017-December},
   year = {2017}
}


@inproceedings{Wang2018,
   author = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
   doi = {10.18653/v1/w18-5446},
   booktitle = {EMNLP 2018 - 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Proceedings of the 1st Workshop},
   title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
   year = {2018}
}

@article{Niu2021,
   abstract = {Attention has arguably become one of the most important concepts in the deep learning field. It is inspired by the biological systems of humans that tend to focus on the distinctive parts when processing large amounts of information. With the development of deep neural networks, attention mechanism has been widely used in diverse application domains. This paper aims to give an overview of the state-of-the-art attention models proposed in recent years. Toward a better general understanding of attention mechanisms, we define a unified model that is suitable for most attention structures. Each step of the attention mechanism implemented in the model is described in detail. Furthermore, we classify existing attention models according to four criteria: the softness of attention, forms of input feature, input representation, and output representation. Besides, we summarize network architectures used in conjunction with the attention mechanism and describe some typical applications of attention mechanism. Finally, we discuss the interpretability that attention brings to deep learning and present its potential future trends.},
   author = {Zhaoyang Niu and Guoqiang Zhong and Hui Yu},
   doi = {10.1016/J.NEUCOM.2021.03.091},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Attention mechanism,Computer vision applications,Convolutional Neural Network (CNN),Deep learning,Encoder-decoder,Natural language processing applications,Recurrent Neural Network (RNN),Unified attention model},
   month = {9},
   pages = {48-62},
   publisher = {Elsevier},
   title = {A review on the attention mechanism of deep learning},
   volume = {452},
   url = {https://www.sciencedirect.com/science/article/abs/pii/S092523122100477X?via%3Dihub},
   year = {2021}
}

@misc{deepseekai2025deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
      year={2025},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@article{Gou2021,
   abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
   author = {Jianping Gou and Baosheng Yu and Stephen J. Maybank and Dacheng Tao},
   doi = {10.1007/s11263-021-01453-z},
   issn = {15731405},
   issue = {6},
   journal = {International Journal of Computer Vision},
   title = {Knowledge Distillation: A Survey},
   volume = {129},
   year = {2021}
}

@inproceedings{Pan2021,
   abstract = {Pre-trained language models have been applied to various NLP tasks with considerable performance gains. However, the large model sizes, together with the long inference time, limit the deployment of such models in real-time applications. One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models. Most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains. We notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation. Hence we propose a Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students. Specifically, we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher. Experiments on public multi-domain NLP tasks show the effectiveness and superiority of the proposed Meta-KD framework. Further, we also demonstrate the capability of Meta-KD in the settings where the training data is scarce.},
   author = {Haojie Pan and Chengyu Wang and Minghui Qiu and Yichang Zhang and Yaliang Li and Jun Huang},
   doi = {10.18653/v1/2021.acl-long.236},
   booktitle = {ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
   title = {Meta-KD: A meta knowledge distillation framework for language model compression across domains},
   volume = {1},
   year = {2021}
}

@inproceedings{Jiao2020,
   abstract = {Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large “teacher” BERT can be effectively transferred to a small “student” TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT41 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ∼28% parameters and ∼31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERTBASE},
   author = {Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
   doi = {10.18653/v1/2020.findings-emnlp.372},
   booktitle = {Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020},
   title = {TinyBERT: Distilling BERT for natural language understanding},
   year = {2020}
}

@inproceedings{Jiang2023,
   abstract = {The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher models to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any “feedback”-identifying challenging instructions where the student model's performance falls short-to boost the student model's proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify “hard” instructions and generate new “hard” instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a student model (named Lion), using a mere 70k training data. Our results show that Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned models like Vicuna-13B by 55.4% in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7% on AGIEval.},
   author = {Yuxin Jiang and Chunkit Chan and Mingyang Chen and Wei Wang},
   doi = {10.18653/v1/2023.emnlp-main.189},
   booktitle = {EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings},
   title = {Lion: Adversarial Distillation of Proprietary Large Language Models},
   year = {2023}
}
